1. Are there any parts of the original WordCount that still confuse you? If so, what?
No. After completing this assignment and constructing similar programs my own, I now have a better understanding about the MapReduce framework.

2. How did the output change when you submitted with -D mapreduce.job.reduces=3? Why would this be necessary if your job produced large output sets?
The output was separated into three files instead of one. This would be necessary for large output set because this allows three reducers work on the job in parallel, which helps to balance the load on each node and lower the cost of failure.

3. How was the -D mapreduce.job.reduces=0 output different?
The output had larger size compared to the one with reducers involved and the output was not sorted. With the number of reducer set to zero, the output was actually the one generated by the mapper and there was no reduction at all.

4. Was there any noticeable difference in the running time of your RedditAverage with and without the combiner optimization?
There was no noticeable difference in the running time with and without the combiner optimization. This was because the dataset for this assignment was relatively small. But logically, if we have a large dataset to work with, the running time with the combiner should be significantly less than the one without the combiner.